# -*- coding: utf-8 -*-
"""employe_attrition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oqiD-8RQMRApjFQQIWvrylXzc_6quBIY

# Notebook: Análise da Satisfação dos Funcionários

## Introdução
Neste projeto, nosso objetivo é analisar a satisfação dos funcionários em uma empresa, utilizando um conjunto de dados que contém informações sobre desempenho, horas trabalhadas, e outros fatores. O objetivo é prever a satisfação dos funcionários com base em suas características.

O conjunto de dados contém informações como identificação dos funcionários, níveis de satisfação, avaliações de desempenho, envolvimento em projetos, entre outros. Essas informações nos permitirão entender os fatores que impactam a satisfação dos funcionários.

## Carga de Dados

### Importação de bibliotecas necessárias
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.utils import resample
from sklearn.model_selection import GridSearchCV

"""## Visualização Básica dos Dados"""

# Carregando o dataset
df = pd.read_csv('/content/Employee Attrition.csv')

# Visualização básica dos dados
df.head(10)

"""## Pré-processamento dos Dados
### Limpeza de dados
"""

# Verificando valores ausentes
df.isnull().sum()

# Tratamento de valores ausentes (se necessário)
df.dropna(inplace=True)

# Remoção de duplicados
df.drop_duplicates(inplace=True)

"""## Separação entre variáveis independentes e dependentes"""

# Supondo que a variável dependente seja 'Satisfaction'
X = df.drop('satisfaction_level', axis=1)

"""## Codificação de variáveis categóricas"""

# Converte variáveis categóricas em variáveis dummy
X_encoded = pd.get_dummies(X, drop_first=True)

# Converte para categórica
y = (df['satisfaction_level'] >= 0.5).astype(int)

"""# Verificando a distribuição das classes antes do balanceamento"""

print("Distribuição das classes antes do balanceamento:")
print(y.value_counts())

"""# Balanceamento das classes"""

y0 = df[df['satisfaction_level'] < 0.5]
y1 = df[df['satisfaction_level'] >= 0.5]

"""# Upsampling da classe minoritária"""

y1_upsampled = resample(y1, replace=True, n_samples=len(y0), random_state=42)
data_balanced = pd.concat([y0, y1_upsampled])

"""# Atualizando as variáveis X e y"""

y_balanced = (data_balanced['satisfaction_level'] >= 0.5).astype(int)
X_balanced = data_balanced.drop(['satisfaction_level'], axis=1)
X_encoded_balanced = pd.get_dummies(X_balanced, drop_first=True)

"""## Divisão dos dados em conjuntos de treino e teste (holdout)"""

X_train, X_test, y_train, y_test = train_test_split(X_encoded_balanced, y_balanced, test_size=0.2, random_state=42)

"""## Transformação de Dados
### Normalização e padronização dos dados
"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""## Modelagem
### Criação de Pipelines para Cada Modelo
"""

# Criação de Pipelines para Cada Modelo
models = {
    'KNN': Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())]),
    'Decision Tree': Pipeline([('scaler', StandardScaler()), ('tree', DecisionTreeClassifier())]),
    'Naive Bayes': Pipeline([('scaler', StandardScaler()), ('nb', GaussianNB())]),
    'SVM': Pipeline([('scaler', StandardScaler()), ('svm', SVC())])
}

results = {}
for model_name, model in models.items():
    model.fit(X_train_scaled, y_train)  # Treinamento do modelo
    y_pred = model.predict(X_test_scaled)  # Predição
    results[model_name] = accuracy_score(y_test, y_pred)  # Avaliação

"""## Otimização de Hiperparâmetros
### Uso de GridSearchCV
"""

# Definindo o grid de hiperparâmetros
param_grid = {
    'KNN': {'knn__n_neighbors': [3, 5, 7]},
    'Decision Tree': {'tree__max_depth': [None, 10, 20]},
    'Naive Bayes': {},  # Naive Bayes não tem hiperparâmetros relevantes para ajuste
    'SVM': {'svm__C': [0.1, 1, 10]}
}

best_models = {}
for model_name, model in models.items():
    # Verifica se há hiperparâmetros a serem ajustados
    if param_grid[model_name]:
        grid = GridSearchCV(model, param_grid[model_name], cv=5)
        grid.fit(X_train_scaled, y_train)
        best_models[model_name] = grid.best_estimator_
    else:
        # Treina o modelo sem ajuste de hiperparâmetros
        model.fit(X_train_scaled, y_train)
        best_models[model_name] = model

"""## Avaliação dos Modelos
### Comparação de Resultados
"""

for model_name, model in best_models.items():
    y_pred = model.predict(X_test_scaled)
    print(f"Model: {model_name}")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("\n")

"""## Exportação do Modelo
### Salvamento do Melhor Modelo
"""

feature_names = X_encoded.columns
joblib.dump(feature_names, 'best_model_employee_feature_names.pkl')

# Salvar o melhor modelo
joblib.dump(best_models['SVM'], 'best_model_employee_attrition.pkl')

# Para teste com PyTest
X_encoded_columns = X_encoded.columns.tolist()
joblib.dump((best_models['SVM'], X_encoded_columns), 'best_model_with_columns.pkl')